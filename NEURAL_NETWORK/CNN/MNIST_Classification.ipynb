{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Normalize the images to values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8274 - loss: 0.1126 - val_accuracy: 0.9641 - val_loss: 0.0231\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9707 - loss: 0.0197 - val_accuracy: 0.9759 - val_loss: 0.0154\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9829 - loss: 0.0120 - val_accuracy: 0.9790 - val_loss: 0.0136\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9888 - loss: 0.0081 - val_accuracy: 0.9786 - val_loss: 0.0132\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9923 - loss: 0.0057 - val_accuracy: 0.9818 - val_loss: 0.0119\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9941 - loss: 0.0043 - val_accuracy: 0.9807 - val_loss: 0.0120\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0029 - val_accuracy: 0.9812 - val_loss: 0.0129\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9973 - loss: 0.0023 - val_accuracy: 0.9828 - val_loss: 0.0136\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9983 - loss: 0.0016 - val_accuracy: 0.9830 - val_loss: 0.0131\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9984 - loss: 0.0015 - val_accuracy: 0.9837 - val_loss: 0.0129\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(28, 28)))\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_probs = tf.nn.softmax(y_pred_probs)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "y_pred_one_hot = to_categorical(y_pred_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9837\n",
      "F1 Macro: 0.9835879140938953\n",
      "F1 Micro: 0.9837\n",
      "F1 Weighted: 0.9836997017099444\n",
      "F1 Samples: 0.9837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_one_hot)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_one_hot, average='macro')\n",
    "f1_micro = f1_score(y_test, y_pred_one_hot, average='micro')\n",
    "f1_weighted = f1_score(y_test, y_pred_one_hot, average='weighted')\n",
    "f1_samples = f1_score(y_test, y_pred_one_hot, average='samples')\n",
    "\n",
    "print(f'F1 Macro: {f1_macro}')\n",
    "print(f'F1 Micro: {f1_micro}')\n",
    "print(f'F1 Weighted: {f1_weighted}')\n",
    "print(f'F1 Samples: {f1_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 27ms/step - accuracy: 0.7517 - loss: 0.1542 - val_accuracy: 0.9793 - val_loss: 0.0158\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - accuracy: 0.9802 - loss: 0.0145 - val_accuracy: 0.9857 - val_loss: 0.0093\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9869 - loss: 0.0098 - val_accuracy: 0.9878 - val_loss: 0.0087\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9902 - loss: 0.0074 - val_accuracy: 0.9883 - val_loss: 0.0080\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9916 - loss: 0.0061 - val_accuracy: 0.9902 - val_loss: 0.0066\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.9936 - loss: 0.0049 - val_accuracy: 0.9912 - val_loss: 0.0058\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.9953 - loss: 0.0039 - val_accuracy: 0.9914 - val_loss: 0.0061\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.9960 - loss: 0.0032 - val_accuracy: 0.9910 - val_loss: 0.0059\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.9964 - loss: 0.0030 - val_accuracy: 0.9909 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9974 - loss: 0.0025 - val_accuracy: 0.9907 - val_loss: 0.0067\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_probs = tf.nn.softmax(y_pred_probs)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "y_pred_one_hot = to_categorical(y_pred_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9907\n",
      "F1 Macro: 0.990580813434416\n",
      "F1 Micro: 0.9907\n",
      "F1 Weighted: 0.9907006061674223\n",
      "F1 Samples: 0.9907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_one_hot)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_one_hot, average='macro')\n",
    "f1_micro = f1_score(y_test, y_pred_one_hot, average='micro')\n",
    "f1_weighted = f1_score(y_test, y_pred_one_hot, average='weighted')\n",
    "f1_samples = f1_score(y_test, y_pred_one_hot, average='samples')\n",
    "\n",
    "print(f'F1 Macro: {f1_macro}')\n",
    "print(f'F1 Micro: {f1_micro}')\n",
    "print(f'F1 Weighted: {f1_weighted}')\n",
    "print(f'F1 Samples: {f1_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAamElEQVR4nO3df2xV9f3H8dfl1wW1vaz2x23lhwUUJr82GJQORQgNbbcQUMbU+QcuBAJczKBTly4T6rakG0umcam4ZAZmJiAkAyJZWLDYks2CASXEMRtKulECLUjCvVCkEPr5/sHXO6+0lHO5t+/29vlIPom993x63xyvPL29l4PPOecEAEA362c9AACgbyJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxADrAb6uvb1dZ86cUVpamnw+n/U4AACPnHO6dOmS8vLy1K9f569zelyAzpw5o+HDh1uPAQC4S01NTRo2bFin9/e4H8GlpaVZjwAASICufj9PWoCqqqr04IMPavDgwSooKNBHH310R/v4sRsApIaufj9PSoDeffddlZWVaf369fr44481efJkFRcX69y5c8l4OABAb+SSYPr06S4UCkW/vnHjhsvLy3OVlZVd7g2Hw04Si8VisXr5CofDt/39PuGvgK5du6YjR46oqKgoelu/fv1UVFSkurq6W45va2tTJBKJWQCA1JfwAH3++ee6ceOGcnJyYm7PyclRc3PzLcdXVlYqEAhEF5+AA4C+wfxTcOXl5QqHw9HV1NRkPRIAoBsk/M8BZWZmqn///mppaYm5vaWlRcFg8Jbj/X6//H5/oscAAPRwCX8FNGjQIE2dOlXV1dXR29rb21VdXa3CwsJEPxwAoJdKypUQysrKtGTJEn3nO9/R9OnT9dprr6m1tVU//vGPk/FwAIBeKCkBeuqpp3T+/HmtW7dOzc3N+ta3vqW9e/fe8sEEAEDf5XPOOeshvioSiSgQCFiPAQC4S+FwWOnp6Z3eb/4pOABA30SAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMDLAeAOjK4sWLPe/Zvn17EibpO374wx963rNjx44kTIJUxisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEFyNNMVVVVZ73PPLII0mYJHGys7O77bHOnz/veU88F+7s6eK5AOyqVauSMEliHD9+PK59oVAowZPgq3gFBAAwQYAAACYSHqCKigr5fL6YNW7cuEQ/DACgl0vKe0Djx4/X+++//78HGcBbTQCAWEkpw4ABAxQMBpPxrQEAKSIp7wGdOHFCeXl5GjVqlJ599lmdOnWq02Pb2toUiURiFgAg9SU8QAUFBdq8ebP27t2rjRs3qrGxUY899pguXbrU4fGVlZUKBALRNXz48ESPBADogRIeoNLSUi1evFiTJk1ScXGx/va3v+nixYvavn17h8eXl5crHA5HV1NTU6JHAgD0QEn/dMDQoUP18MMPq6GhocP7/X6//H5/sscAAPQwSf9zQJcvX9bJkyeVm5ub7IcCAPQiCQ/QCy+8oNraWv3nP//Rhx9+qCeeeEL9+/fXM888k+iHAgD0Ygn/Edzp06f1zDPP6MKFC8rKytKjjz6qgwcPKisrK9EPBQDoxRIeoG3btiX6W8KDb3/72573FBYWxvVYf//73z3vOXjwYFyP1V3C4bDnPTU1NYkfxFg8/8MYz4VcZ8yY4XlPcXGx5z1jxozxvEeK79cUj4qKim55nJ6Ga8EBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZ8zjlnPcRXRSIRBQIB6zF6raqqKs97Hnnkkbge64033vC8Z8eOHXE9FlLT4sWLPe9ZtWqV5z3p6eme90jSlClT4trnlc/n65bH6W7hcPi2555XQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDB1bABpLwZM2bEta+uri7Bk3SMq2EDANCNCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmBlgPAKDvWrx4sec9q1at8rwnPT3d8x4kH6+AAAAmCBAAwITnAB04cEDz589XXl6efD6fdu3aFXO/c07r1q1Tbm6uhgwZoqKiIp04cSJR8wIAUoTnALW2tmry5Mmqqqrq8P4NGzbo9ddf15tvvqlDhw7p3nvvVXFxsa5evXrXwwIAUofnDyGUlpaqtLS0w/ucc3rttdf0i1/8QgsWLJAkvf3228rJydGuXbv09NNP3920AICUkdD3gBobG9Xc3KyioqLobYFAQAUFBaqrq+twT1tbmyKRSMwCAKS+hAaoublZkpSTkxNze05OTvS+r6usrFQgEIiu4cOHJ3IkAEAPZf4puPLycoXD4ehqamqyHgkA0A0SGqBgMChJamlpibm9paUlet/X+f1+paenxywAQOpLaIDy8/MVDAZVXV0dvS0SiejQoUMqLCxM5EMBAHo5z5+Cu3z5shoaGqJfNzY26ujRo8rIyNCIESO0Zs0a/frXv9ZDDz2k/Px8vfzyy8rLy9PChQsTOTcAoJfzHKDDhw9rzpw50a/LysokSUuWLNHmzZv10ksvqbW1VcuXL9fFixf16KOPau/evRo8eHDipgYA9Ho+55yzHuKrIpGIAoGA9RhAj7N27VrPe3r6f0szZszwvKe4uNjzntOnT3veI0lvvfVWXPu8qqio6JbH6W7hcPi27+ubfwoOANA3ESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwARXw0aPl5WV5XnP+PHjkzCJre3bt3veE8+5k6Tjx4973nPu3Lm4Hqs7xPPrkaRQKJTgSfoWroYNAOiRCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATA6wHALoye/Zsz3viuXAn/qeiosLznh07diR+EKQ0XgEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GCmAW3TXxVxfffVVz3vKysqSMAks8AoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDhc8456yG+KhKJKBAIWI+BHiQrK8vznvHjxydhkr5j/fr1nvfMnj3b857Tp0973tPQ0OB5z/Hjxz3vkaRQKBTXPtwUDoeVnp7e6f28AgIAmCBAAAATngN04MABzZ8/X3l5efL5fNq1a1fM/c8995x8Pl/MKikpSdS8AIAU4TlAra2tmjx5sqqqqjo9pqSkRGfPno2urVu33tWQAIDU4/lvRC0tLVVpaeltj/H7/QoGg3EPBQBIfUl5D6impkbZ2dkaO3asVq5cqQsXLnR6bFtbmyKRSMwCAKS+hAeopKREb7/9tqqrq/Xb3/5WtbW1Ki0t1Y0bNzo8vrKyUoFAILqGDx+e6JEAAD2Q5x/BdeXpp5+O/vPEiRM1adIkjR49WjU1NZo7d+4tx5eXl6usrCz6dSQSIUIA0Ack/WPYo0aNUmZmZqd/eMzv9ys9PT1mAQBSX9IDdPr0aV24cEG5ubnJfigAQC/i+Udwly9fjnk109jYqKNHjyojI0MZGRl65ZVXtGjRIgWDQZ08eVIvvfSSxowZo+Li4oQODgDo3TwH6PDhw5ozZ0706y/fv1myZIk2btyoY8eO6c9//rMuXryovLw8zZs3T7/61a/k9/sTNzUAoNfjYqQAbrF48WLPe+K5AOyMGTM874nnpynxXPRUkt5666249nlVUVHRLY/T3bgYKQCgRyJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJroYNwEw8V91etWqV5z3x/k3LU6ZMiWufVz6fr1sep7txNWwAQI9EgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgYqQAUt6MGTPi2ldXV5fgSTrGxUgBAOhGBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAlPAaqsrNS0adOUlpam7OxsLVy4UPX19THHXL16VaFQSPfff7/uu+8+LVq0SC0tLQkdGgDQ+3kKUG1trUKhkA4ePKh9+/bp+vXrmjdvnlpbW6PHrF27Vu+995527Nih2tpanTlzRk8++WTCBwcA9G4+55yLd/P58+eVnZ2t2tpazZo1S+FwWFlZWdqyZYt+8IMfSJI+++wzffOb31RdXZ1mzJjR5feMRCIKBALxjgQAt7iT33s6UldXl+BJOubz+brlcbpbOBxWenp6p/ff1XtA4XBYkpSRkSFJOnLkiK5fv66ioqLoMePGjdOIESM6/RfZ1tamSCQSswAAqS/uALW3t2vNmjWaOXOmJkyYIElqbm7WoEGDNHTo0Jhjc3Jy1Nzc3OH3qaysVCAQiK7hw4fHOxIAoBeJO0ChUEiffvqptm3bdlcDlJeXKxwOR1dTU9NdfT8AQO8wIJ5Nq1ev1p49e3TgwAENGzYsenswGNS1a9d08eLFmFdBLS0tCgaDHX4vv98vv98fzxgAgF7M0ysg55xWr16tnTt3av/+/crPz4+5f+rUqRo4cKCqq6ujt9XX1+vUqVMqLCxMzMQAgJTg6RVQKBTSli1btHv3bqWlpUXf1wkEAhoyZIgCgYCWLl2qsrIyZWRkKD09Xc8//7wKCwvj/hQKACA1eQrQxo0bJUmzZ8+OuX3Tpk167rnnJEmvvvqq+vXrp0WLFqmtrU3FxcV64403EjIsACB13NWfA0qGVP1zQBUVFd3yOH/605887zl9+nQSJgG6Fs9PRkpKSjzv+ep71V4sXbo0rn1e8eeAAADoRgQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADAR19+ICu8ef/zxbnmchoYGz3u4GjasLF682POeVatWed4TiUQ875GkmpqauPbhzvAKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIu8mcOXO65XE+/PBDz3sKCwuTMAnQc/zrX/+Ka193/XfbV/EKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIU8x3v/td6xEA4I7wCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY8BSgyspKTZs2TWlpacrOztbChQtVX18fc8zs2bPl8/li1ooVKxI6NACg9/MUoNraWoVCIR08eFD79u3T9evXNW/ePLW2tsYct2zZMp09eza6NmzYkNChAQC9n6e/EXXv3r0xX2/evFnZ2dk6cuSIZs2aFb39nnvuUTAYTMyEAICUdFfvAYXDYUlSRkZGzO3vvPOOMjMzNWHCBJWXl+vKlSudfo+2tjZFIpGYBQDoA1ycbty44b7//e+7mTNnxtz+xz/+0e3du9cdO3bM/eUvf3EPPPCAe+KJJzr9PuvXr3eSWCwWi5ViKxwO37YjcQdoxYoVbuTIka6pqem2x1VXVztJrqGhocP7r1696sLhcHQ1NTWZnzQWi8Vi3f3qKkCe3gP60urVq7Vnzx4dOHBAw4YNu+2xBQUFkqSGhgaNHj36lvv9fr/8fn88YwAAejFPAXLO6fnnn9fOnTtVU1Oj/Pz8LvccPXpUkpSbmxvXgACA1OQpQKFQSFu2bNHu3buVlpam5uZmSVIgENCQIUN08uRJbdmyRd/73vd0//3369ixY1q7dq1mzZqlSZMmJeUXAADopby876NOfs63adMm55xzp06dcrNmzXIZGRnO7/e7MWPGuBdffLHLnwN+VTgcNv+5JYvFYrHufnX1e7/v/8PSY0QiEQUCAesxAAB3KRwOKz09vdP7uRYcAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEjwuQc856BABAAnT1+3mPC9ClS5esRwAAJEBXv5/7XA97ydHe3q4zZ84oLS1NPp8v5r5IJKLhw4erqalJ6enpRhPa4zzcxHm4ifNwE+fhpp5wHpxzunTpkvLy8tSvX+evcwZ040x3pF+/fho2bNhtj0lPT+/TT7AvcR5u4jzcxHm4ifNwk/V5CAQCXR7T434EBwDoGwgQAMBErwqQ3+/X+vXr5ff7rUcxxXm4ifNwE+fhJs7DTb3pPPS4DyEAAPqGXvUKCACQOggQAMAEAQIAmCBAAAATvSZAVVVVevDBBzV48GAVFBToo48+sh6p21VUVMjn88WscePGWY+VdAcOHND8+fOVl5cnn8+nXbt2xdzvnNO6deuUm5urIUOGqKioSCdOnLAZNom6Og/PPffcLc+PkpISm2GTpLKyUtOmTVNaWpqys7O1cOFC1dfXxxxz9epVhUIh3X///brvvvu0aNEitbS0GE2cHHdyHmbPnn3L82HFihVGE3esVwTo3XffVVlZmdavX6+PP/5YkydPVnFxsc6dO2c9WrcbP368zp49G13/+Mc/rEdKutbWVk2ePFlVVVUd3r9hwwa9/vrrevPNN3Xo0CHde++9Ki4u1tWrV7t50uTq6jxIUklJSczzY+vWrd04YfLV1tYqFArp4MGD2rdvn65fv6558+aptbU1eszatWv13nvvaceOHaqtrdWZM2f05JNPGk6deHdyHiRp2bJlMc+HDRs2GE3cCdcLTJ8+3YVCoejXN27ccHl5ea6ystJwqu63fv16N3nyZOsxTElyO3fujH7d3t7ugsGg+93vfhe97eLFi87v97utW7caTNg9vn4enHNuyZIlbsGCBSbzWDl37pyT5Gpra51zN//dDxw40O3YsSN6zL///W8nydXV1VmNmXRfPw/OOff444+7n/zkJ3ZD3YEe/wro2rVrOnLkiIqKiqK39evXT0VFRaqrqzOczMaJEyeUl5enUaNG6dlnn9WpU6esRzLV2Nio5ubmmOdHIBBQQUFBn3x+1NTUKDs7W2PHjtXKlSt14cIF65GSKhwOS5IyMjIkSUeOHNH169djng/jxo3TiBEjUvr58PXz8KV33nlHmZmZmjBhgsrLy3XlyhWL8TrV4y5G+nWff/65bty4oZycnJjbc3Jy9NlnnxlNZaOgoECbN2/W2LFjdfbsWb3yyit67LHH9OmnnyotLc16PBPNzc2S1OHz48v7+oqSkhI9+eSTys/P18mTJ/Xzn/9cpaWlqqurU//+/a3HS7j29natWbNGM2fO1IQJEyTdfD4MGjRIQ4cOjTk2lZ8PHZ0HSfrRj36kkSNHKi8vT8eOHdPPfvYz1dfX669//avhtLF6fIDwP6WlpdF/njRpkgoKCjRy5Eht375dS5cuNZwMPcHTTz8d/eeJEydq0qRJGj16tGpqajR37lzDyZIjFArp008/7RPvg95OZ+dh+fLl0X+eOHGicnNzNXfuXJ08eVKjR4/u7jE71ON/BJeZman+/fvf8imWlpYWBYNBo6l6hqFDh+rhhx9WQ0OD9ShmvnwO8Py41ahRo5SZmZmSz4/Vq1drz549+uCDD2L++pZgMKhr167p4sWLMcen6vOhs/PQkYKCAknqUc+HHh+gQYMGaerUqaquro7e1t7erurqahUWFhpOZu/y5cs6efKkcnNzrUcxk5+fr2AwGPP8iEQiOnToUJ9/fpw+fVoXLlxIqeeHc06rV6/Wzp07tX//fuXn58fcP3XqVA0cODDm+VBfX69Tp06l1POhq/PQkaNHj0pSz3o+WH8K4k5s27bN+f1+t3nzZnf8+HG3fPlyN3ToUNfc3Gw9Wrf66U9/6mpqalxjY6P75z//6YqKilxmZqY7d+6c9WhJdenSJffJJ5+4Tz75xElyv//9790nn3zi/vvf/zrnnPvNb37jhg4d6nbv3u2OHTvmFixY4PLz890XX3xhPHli3e48XLp0yb3wwguurq7ONTY2uvfff99NmTLFPfTQQ+7q1avWoyfMypUrXSAQcDU1Ne7s2bPRdeXKlegxK1ascCNGjHD79+93hw8fdoWFha6wsNBw6sTr6jw0NDS4X/7yl+7w4cOusbHR7d69240aNcrNmjXLePJYvSJAzjn3hz/8wY0YMcINGjTITZ8+3R08eNB6pG731FNPudzcXDdo0CD3wAMPuKeeeso1NDRYj5V0H3zwgZN0y1qyZIlz7uZHsV9++WWXk5Pj/H6/mzt3rquvr7cdOgludx6uXLni5s2b57KystzAgQPdyJEj3bJly1Luf9I6+vVLcps2bYoe88UXX7hVq1a5b3zjG+6ee+5xTzzxhDt79qzd0EnQ1Xk4deqUmzVrlsvIyHB+v9+NGTPGvfjiiy4cDtsO/jX8dQwAABM9/j0gAEBqIkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBM/B8+dY9r73vTCwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted label: 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Đọc ảnh từ đường dẫn\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Kiểm tra xem ảnh có được đọc thành công không\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image from path: {image_path}\")\n",
    "    \n",
    "    # Ngưỡng nhị phân ảnh\n",
    "    _, img_thresh = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Tìm viền của ảnh\n",
    "    contours, _ = cv2.findContours(img_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Lấy viền lớn nhất\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        img_cropped = img_thresh[y:y+h, x:x+w]\n",
    "    else:\n",
    "        img_cropped = img_thresh  # Nếu không tìm thấy viền, giữ nguyên ảnh\n",
    "\n",
    "    # Thay đổi kích thước ảnh về 20x20 pixel (giống kích thước trong MNIST)\n",
    "    img_resized = cv2.resize(img_cropped, (20, 20), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Tạo khung nền 28x28 và đặt ảnh vào giữa\n",
    "    img_padded = np.zeros((28, 28), dtype=np.uint8) * 255  # Nền đen\n",
    "    x_offset = (28 - 20) // 2\n",
    "    y_offset = (28 - 20) // 2\n",
    "    img_padded[y_offset:y_offset+20, x_offset:x_offset+20] = img_resized\n",
    "    \n",
    "    # Chuẩn hóa giá trị ảnh (MNIST có các giá trị pixel từ 0 đến 1)\n",
    "    img_normalized = img_padded / 255.0\n",
    "    \n",
    "    return img_normalized\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "image_path = 'So3.png' # Thay bằng đường dẫn tới ảnh chữ viết tay\n",
    "processed_image = preprocess_image(image_path)\n",
    "\n",
    "# Kiểm tra ảnh sau khi tiền xử lý\n",
    "plt.imshow(processed_image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Expand dimensions to match the input shape expected by the model\n",
    "processed_image = np.expand_dims(processed_image, axis=0)  # Add batch dimension\n",
    "processed_image = np.expand_dims(processed_image, axis=-1)  # Add channel dimension\n",
    "\n",
    "# Predict the class of the input image using the CNN model\n",
    "predicted_probs = model.predict(processed_image)\n",
    "predicted_probs = tf.nn.softmax(predicted_probs)\n",
    "predicted_label = np.argmax(predicted_probs, axis=1)\n",
    "\n",
    "print(f'Predicted label: {predicted_label[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
